{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on workflow generation with PyCBC\n",
    "\n",
    "## Workflows\n",
    "\n",
    "*Pipeline* specifies a set of scientific tasks to be executed. *Workflow* specifies an instance of a pipeline with settings or configuration to execute a particular analysis.\n",
    "\n",
    "A cononical example is the diamond workflow, an input file `f.a` goes to a process `preprocess` which creates two files `f.b1` and `f.b2`. Each of those files (`f.b1` and `f.b2`) gets run with the process `findrange`. A final process `analyze` takes the output from both `findrange` processes and produces a final file called `f.d`. A graphical depiction is shown below (Image credit: Pegasus documentation).\n",
    "\n",
    "<img src=\"img_diamond_wf.png\" alt=\"workflow_image\" style=\"width: 200px;\"/>\n",
    "\n",
    "Generating and executing workflows are very common in scientific computing, and there is an active field how to manage the execution of workflows using *workflow management systems*:\n",
    "  * A workflow management system: Automatically locates the necessary input data and computational resources, and manages storage space for executing data-intensive workflows on storage-constrained resources.\n",
    "\n",
    "In PyCBC, we have made the choice to use the [Pegasus](https://pegasus.isi.edu/) workflow management system which uses [HTCondor](https://research.cs.wisc.edu/htcondor/) scheduler. For a overview of different workflow management systems see: [R. F. Silva et al. 2017](https://www.sciencedirect.com/science/article/pii/S0167739X17302510)\n",
    "\n",
    "In this tutorial we will:\n",
    "  * Demonstrate how to generate a workflow to execute PyCBC Inference and some plotting codes with PyCBC.\n",
    "  * Demonstrate how to generate a workflow to execute a study using PyCBC Inference for a population of simulated binary black hole mergers in simulated data.\n",
    "  * An example of creating a simple workflow in PyCBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup: SciServer terminal and acquire tutorial\n",
    "\n",
    "Eariler, you went through how to estimate the posteriors with PyCBC Inference on the command line and several plots. There are workflows that execute a workflow that: analyzes these signals and generates a set of plots on HTML pages.\n",
    "\n",
    "We will run the example on the SciServer. To setup a container with a terminal do:\n",
    "\n",
    "  1. Go to compute URL at: https://apps.sciserver.org/compute/\n",
    "  2. Click `Create Container`.\n",
    "  3. Give the container a name and select the `Python + R` image from the menu.\n",
    "  4. Click on the name of the container that should now appear on the list at: https://apps.sciserver.org/compute/\n",
    "  5. A Jupyter notebook should appear. Click `New`->`Terminal`.\n",
    "\n",
    "You should now have a terminal open.\n",
    "\n",
    "Create an Anaconda environment with:\n",
    "```\n",
    "conda create --yes --name pycbc-2p7 python=2.7\n",
    "```\n",
    "\n",
    "Load the new Anaconda environment called `pycbc-2p7` with:\n",
    "```\n",
    "source activate pycbc-2p7\n",
    "```\n",
    "\n",
    "Install prerequisites with:\n",
    "```\n",
    "# install LALSuite and PyCBC\n",
    "pip install lalsuite pycbc\n",
    "# install Pegasus Python API\n",
    "pip install http://download.pegasus.isi.edu/pegasus/4.8.1/pegasus-python-source-4.8.1.tar.gz\n",
    "```\n",
    "\n",
    "To test your install, the following commands should work:\n",
    "```\n",
    "python -c \"import pycbc\"\n",
    "python -c \"import Pegasus\"\n",
    "```\n",
    "\n",
    "Now, get data for this tutorial in the workspace with:\n",
    "```\n",
    "cd workspace\n",
    "git clone http://github.com/gwastro/PyCBCInferenceWorkshopMay2019.git\n",
    "```\n",
    "\n",
    "And change into this tutorial with:\n",
    "```\n",
    "cd PyCBCInferenceWorkshopMay2019/pycbc_inference_workflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating workflow for binary black hole with GWOSC data\n",
    "\n",
    "We will use data files from [GWSOC](https://www.gw-openscience.org) for GW150914, and run a workflow that uses ``pycbc_inference`` and generates a set of plots in HTML pages. There is a script already in the repository that demonstrates how to generate the workflow. \n",
    "\n",
    "This script does the following:\n",
    "  1. Download two data files from GWSOC with `wget`.\n",
    "  1. Set varables associated with the data, eg. trigger time, similar to the previous tutorals.\n",
    "  1. Make a call to `pycbc_make_inference_workflow` which is the *workflow generator*. It reads in configuration files that describe how to run `pycbc_inference` and it will output an abstract representation of the workflow.\n",
    "\n",
    "To execute this script do:\n",
    "```\n",
    "bash run_ex_bbh.sh\n",
    "```\n",
    "\n",
    "You should notice a number of log messages in the terminal after executing this script and a new directory called `bbh` is created.\n",
    " \n",
    "Looking inside, we see `pycbc_make_inference_workflow` takes the following options:\n",
    "  * `--workflow-name`: A short name to identify this workflow.\n",
    "  * `--config-file`: A workflow configuration file.\n",
    "  * `--inference-config-file`: A PyCBC Inference configuration file, e.g. from the eariler tutorials.\n",
    "  * `--output-map`: Maps logical file names to physical file names on the filesystem. \n",
    "  * `--transformation-catalog`: Maps logical executable names to physical executables on the system. E.g. in configuration file references to `inference` get mapped to `/your/path/to/pycbc_inference`.\n",
    "  * `--gps-end-time`: Event's coalescence time as reported from search or *a priori* estimate.\n",
    "  * `--config-overrides`: A listing of options in the workflow file that will be overwritten from the command line.\n",
    " \n",
    "## Running workflow for binary balck hole mergers\n",
    " \n",
    "Note the SciServer container does not contain a HTCondor cluster which is a requirement. In order to execute the workflow (in the prescence) of a cluster. **Therefore, SciServer cannot run this command to completion.** But to submit, one would do:\n",
    "```\n",
    "cd ${OUTPUT_DIR}\n",
    "pycbc_submit_dax \\\n",
    "   --no-create-proxy \\\n",
    "   --no-grid \\\n",
    "   --dax ${WORKFLOW_NAME}.dax \\\n",
    "   --enable-shared-filesystem \\\n",
    "   --force-no-accounting-group\n",
    "```\n",
    "\n",
    "This command (``pycbc_submit_dax``) will submit the workflow to the HTCondor cluster for execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input: Workflow configuration file\n",
    "\n",
    "The workflow configuration file has several sections:\n",
    "  * ``[workflow]``: Information about the workflow.\n",
    "  * ``[workflow-*]``: Controls certain aspects of the workflow. E.g. ``[workflow-inference]`` gives details on what to plot.\n",
    "  * ``[executables]``: Gives path to executables to use.\n",
    "  * ``[*]``: The remaining sections give command line options to the executes. Names of these sections are the logical names given as keys in ``[executables]``.\n",
    "  * ``[pegasus_profile-*]``: Gives options to Pegasus on what type of resources to acquire. E.g. how many nodes or memory requirements.\n",
    "\n",
    "So for example, ``pycbc_inference`` options are set with:\n",
    "```\n",
    "[executables]\n",
    "inference = ${which:pycbc_inference}\n",
    "\n",
    "[inference]\n",
    "seed = 39392\n",
    "sample-rate = 2048\n",
    "data-conditioning-low-freq = 20\n",
    "strain-high-pass = 15\n",
    "pad-data = 8\n",
    "psd-estimation = median\n",
    "psd-segment-length = 16\n",
    "psd-segment-stride = 8\n",
    "psd-inverse-length = 4\n",
    "```\n",
    "\n",
    "And recall, we added overrides on the command line. The corresponding options added to ``pycbc_inference`` were:\n",
    "```\n",
    "                       inference:psd-start-time:${PSD_START_TIME} \\\n",
    "                       inference:psd-end-time:${PSD_END_TIME} \\\n",
    "                       inference:psd-segment-length:${PSD_SEG_LEN} \\\n",
    "                       inference:psd-segment-stride:${PSD_STRIDE} \\\n",
    "                       inference:psd-inverse-length:${PSD_INVLEN} \\\n",
    "                       inference:nprocesses:${NPROCS} \\\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: DAX\n",
    "\n",
    "When we ran our workflow generator we put all the parts together to describe how the workflow should be constructed.\n",
    "\n",
    "One of the output files is called a *Directed Acyclic Graph in XML* (DAX). The DAX is an XML file that gives an abstract representation of the workflow. It does not specify the paths to files or executables, but rather gives logical names to represent those files.\n",
    "\n",
    "There are several XML elements:\n",
    "  * ``<file>``: Describes an input file to the workflow. Here, you should notice the frame files (data) and the configuration file.\n",
    "  * ``<job>``: Describes how a node will be run. E.g. the command line options.\n",
    "  * ``<child>``: Give parent-child relationships.\n",
    "  \n",
    "In this example, there subworkflows--(1) one that analyzes and plots, and (2) one that generates a HTML page. The primary workflow's DAX is ``bbh/event.dax``.\n",
    "\n",
    "The DAX which contains the ``pycbc_inference`` node is in ``bbh/event-main.dax``. Do:\n",
    "```\n",
    "less bbh/event-main.dax\n",
    "```\n",
    "\n",
    "You should notice the input files, jobs, and parent-child relationships.\n",
    "\n",
    "Do you see your ``[peagusus_profile-inference]`` options? These will be stored in the transformation catalog, eg. ``bbh/event-main.tc.txt``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the workflow output\n",
    "\n",
    "**Note we have only created the logical representation of the workflow, we have not executed the workflow.**\n",
    "\n",
    "We will not be able to execute the workflow in the SciServer container. It requires a HTCondor cluster. At the end of this tutorial I give instructions to download a virtual machine with a single HTCondor node that this workflow can be executed inside.\n",
    "\n",
    "However, I have attached an example set of HTML pages that these workflow generates at [Results](example_pages/index.html).\n",
    "\n",
    "There are several tabs to view different information:\n",
    "  * ``Summary``: Gives an overview of the results. Notice the table of credible intervals and the corner plot of the samples of the posterior.\n",
    "  * ``Detector Sensitivity``: Shows the power spectral density.\n",
    "  * ``Priors``: Samples of the priors used in the analysis.\n",
    "  * ``Posteriors``: Posteriors to be plotted denoted in the ``[workflow-inference]`` section.\n",
    "  * ``Samples``: The chains of the samples from the analysis.\n",
    "  * ``Workflow``: Configuration files and logs from running workflow.\n",
    "  \n",
    "The caption buttons below images or tables gives a description of the file. The command line button gives the command line used to generate the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running example workflow for simulated population of binary black hole mergers\n",
    "\n",
    "There is also an example to generate an workflow that generates and analyzes a population of simulated binary black hole mergers.\n",
    "\n",
    "To run this example, use:\n",
    "```\n",
    "bash run_ex_pp.sh\n",
    "```\n",
    "\n",
    "This script is similar to our previous example, in that it calls the workflow generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your own workflow generator\n",
    "\n",
    "Workflow generators like ``pycbc_make_inference_workflow`` and ``pycbc_make_inference_inj_workflow`` use building blocks within PyCBC's workflow module to build the analysis.\n",
    "\n",
    "Inside the ``pycbc.workflow`` subpackage there are several classes:\n",
    "  * ``Workflow``: Represents the workflow and tracks parent-child relationships.\n",
    "  * ``Executable``: Represents an executable, excluding the command line options.\n",
    "  * ``Node``: Represents an executable with a particular set of command line options.\n",
    "  * ``File``: Represents a file.\n",
    "\n",
    "There is a scirpt ``create_workflow.py`` that contains an example. It creates a simple workflow with one node that runs ``echo``. We go through it here.\n",
    "\n",
    "Import modules. We need the ``pycbc.workflow`` modules for building the workflow. The ``os`` module is used to :\n",
    "```\n",
    "#! /usr/bin/env python\n",
    "\n",
    "from pycbc import workflow as wf\n",
    "```\n",
    "\n",
    "Create a Python object that contains options for the ``Workflow`` class. These are the set of required options the class must have; denoting its short name, configuration file, command line overrides, desired output map path, and desired transofrmation catalog path. To create the class, do:\n",
    "```\n",
    "# options\n",
    "class Options:\n",
    "    pass\n",
    "opts = Options\n",
    "opts.workflow_name = \"wfex\"\n",
    "opts.config_files = [\"ex_wfex.ini\"]\n",
    "opts.config_delete = None\n",
    "opts.config_overrides = None\n",
    "opts.output_file = \"wfex.dax\"\n",
    "opts.output_map = \"output.map\"\n",
    "opts.transformation_catalog = \"wfex.tc.txt\"\n",
    "```\n",
    "\n",
    "Create directory where output files will be written:\n",
    "```\n",
    "# set output directory\n",
    "out_dir = \"wfex_results\"\n",
    "\n",
    "# create output directory\n",
    "wf.makedir(out_dir)\n",
    "```\n",
    "\n",
    "Create an instance of the ``Workflow`` class:\n",
    "```\n",
    "# create workflow\n",
    "container = wf.Workflow(opts, opts.workflow_name)\n",
    "```\n",
    "\n",
    "In our workflow, we will simply call ``echo``. To create the ``Executable`` instance, do:\n",
    "```\n",
    "# create executable\n",
    "exe_echo = wf.Executable(container.cp, \"echo\",\n",
    "                         ifos=container.ifos, out_dir=out_dir)\n",
    "```\n",
    "\n",
    "To create a ``Node`` instance with its command line options, do:\n",
    "```\n",
    "# create node\n",
    "node_echo = exe_echo.create_node()\n",
    "\n",
    "# add options\n",
    "node_echo.add_opt(\"--option-1\", container.analysis_time[0])\n",
    "node_echo.add_opt(\"--option-2\", container.analysis_time[1])\n",
    "```\n",
    "\n",
    "To add the node to the workflow, do:\n",
    "```\n",
    "# add node to workflow\n",
    "container += node_echo\n",
    "```\n",
    "\n",
    "Finally, you will want to write your DAX, output map, and transformation catalogs. Therefore, do:\n",
    "```\n",
    "# save\n",
    "container.save(filename=opts.output_file,\n",
    "               output_map_path=opts.output_map,\n",
    "               transformation_catalog_path=opts.transformation_catalog)\n",
    "```\n",
    "\n",
    "You should now see:\n",
    "  * A new DAX file called ``wfex.dax`` which contains our ``echo`` node.\n",
    "  * A new transformation catalog that displays your physical path to ``echo``.\n",
    "  \n",
    "Look in the configuration file, ``ex_wfex.ini``. This contains the minimum for using the workflow modules as well:\n",
    "```\n",
    "[workflow]\n",
    "start-time = 0\n",
    "end-time = 1\n",
    "\n",
    "[workflow-ifos]\n",
    "h1 =\n",
    "l1 =\n",
    "\n",
    "[executables]\n",
    "echo = ${which:echo}\n",
    "\n",
    "[echo]\n",
    "```\n",
    "\n",
    "This should look familar, eg. notice the ``[workflow]``, `[workflow-*]``, ``[executables]``, and the ``[*]`` sections as with the examples above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional exercises\n",
    "\n",
    "### Get virutal machine with HTCondor node\n",
    "\n",
    "This is a more advanced example, where there is a OVA file you can download and run a virtual machine with a virtual HTCondor cluster. The documentation from Pegsus is [here](https://pegasus.isi.edu/documentation/vm_virtualbox.php).\n",
    "\n",
    "On your local machine, try doing:\n",
    "  * Install VirtualBox\n",
    "  * Download the virtual machine at: http://download.pegasus.isi.edu/pegasus/4.9.1/PegasusTutorialVM-4.9.1.ova\n",
    "  * Launch VirtualBox\n",
    "  * ``File``->``Import Appliance``, and then select your downloaded OVA file.\n",
    "  * Double-click new icon for ``PegasusTutorialVM-4.9.1`` in VirutalBox to launch the virtual machine.\n",
    "\n",
    "This virtual machine does not have all the requirements to run the example. You will need to:\n",
    "  * Install pip, eg. ``curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python get-pip.py --user``\n",
    "  * Install the developmental Python pacakge to obtain the Python header file, eg. ``sudo yum install -y python-devel``.\n",
    "  * Install git, eg. ``sudo yum install -y git``\n",
    "  * Install PyCBC and LALSuite and get the tutorial, eg. ``pip install lalsuite pycbc`` and ``\n",
    "git clone http://github.com/gwastro/PyCBCInferenceWorkshopMay2019.git``.\n",
    "  * You will not be able to submit from ``/tmp``, so for ``pycbc_submit_dax`` use the ``--local-dir`` option. Eg. use ``mkdir -p ${HOME}/tmp && pycbc_submit_dax --local-dir ${HOME}/tmp``.\n",
    "  * Remove the ``[inference-pegasus_profile]`` options. In the virtual machine you will only have a single node, and if you request >1 nodes, your ``pycbc_inference`` node will not run.\n",
    "  * You may need to use ``numpy==1.16.1``.\n",
    "\n",
    "Can you:\n",
    "  * Run the examples in the virtual machine?\n",
    "  * Design your own workflow generator and exevute your own workflow in the virtual machine or a HPC cluster?\n",
    "  * Add dependencies or ``File`` instances to your workflow generator? See the executables in the PyCBC repository ``bin/workflow`` as examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
